{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "In this assignment you will build a language model for the [OHHLA corpus](http://ohhla.com/) we are using in the book. You will train the model on the available training set, and can tune it on the development set. After submission we will run your notebook on a different test set. Your mark will depend on \n",
    "\n",
    "* whether your language model is **properly normalized**,\n",
    "* its **perplexity** on the unseen test set,\n",
    "* your **description** of your approach. \n",
    "\n",
    "To develop your model you have access to:\n",
    "\n",
    "* The training and development data in `data/ohhla`.\n",
    "* The code of the lecture, stored in a python module [here](/edit/statnlpbook/lm.py).\n",
    "* Libraries on the [docker image](https://github.com/uclmr/stat-nlp-book/blob/python/Dockerfile) which contains everything in [this image](https://github.com/jupyter/docker-stacks/tree/master/scipy-notebook), including scikit-learn and tensorflow. \n",
    "\n",
    "As we have to run the notebooks of all students, and because writing efficient code is important, **your notebook should run in 5 minutes at most**, on your machine. Further comments:\n",
    "\n",
    "* We have tested a possible solution on the Azure VMs and it ran in seconds, so it is possible to train a reasonable LM on the data in reasonable time. \n",
    "\n",
    "* Try to run your parameter optimisation offline, such that in your answer notebook the best parameters are already set and don't need to be searched."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Instructions\n",
    "It is important that this file is placed in the **correct directory**. It will not run otherwise. The correct directory is\n",
    "\n",
    "    DIRECTORY_OF_YOUR_BOOK/assignments/2016/assignment1/problem/\n",
    "    \n",
    "where `DIRECTORY_OF_YOUR_BOOK` is a placeholder for the directory you downloaded the book to. After you placed it there, **rename the file** to your UCL ID (of the form `ucxxxxx`). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Instructions\n",
    "This notebook will be used by you to provide your solution, and by us to both assess your solution and enter your marks. It contains three types of sections:\n",
    "\n",
    "1. **Setup** Sections: these sections set up code and resources for assessment. **Do not edit these**. \n",
    "2. **Assessment** Sections: these sections are used for both evaluating the output of your code, and for markers to enter their marks. **Do not edit these**. \n",
    "3. **Task** Sections: these sections require your solutions. They may contain stub code, and you are expected to edit this code. For free text answers simply edit the markdown field.  \n",
    "\n",
    "Note that you are free to **create additional notebook cells** within a task section. \n",
    "\n",
    "Please **do not share** this assignment publicly, by uploading it online, emailing it to friends etc. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Instructions\n",
    "\n",
    "To submit your solution:\n",
    "\n",
    "* Make sure that your solution is fully contained in this notebook. \n",
    "* **Rename this notebook to your UCL ID** (of the form \"ucxxxxx\"), if you have not already done so.\n",
    "* Download the notebook in Jupyter via *File -> Download as -> Notebook (.ipynb)*.\n",
    "* Upload the notebook to the Moodle submission site.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='green'>Setup 1</font>: Load Libraries\n",
    "This cell loads libraries important for evaluation and assessment of your model. **Do not change it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#! SETUP 1\n",
    "import sys, os\n",
    "_snlp_book_dir = \"../../../../\"\n",
    "sys.path.append(_snlp_book_dir) \n",
    "import statnlpbook.lm as lm\n",
    "import statnlpbook.ohhla as ohhla\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='green'>Setup 2</font>: Load Training Data\n",
    "\n",
    "This cell loads the training data. We use this data for assessment to define the reference vocabulary: the union of the words of the training and set set. You can use the dataset to train your model, but you are also free to load the data in a different way, or focus on subsets etc. However, when you do this, still **do not edit this setup section**. Instead refer to the variables in your own code, and slice and dice them as you see fit.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not load ../../../..//data/ohhla/train/www.ohhla.com/anonymous/nas/distant/tribal.nas.txt.html\n"
     ]
    }
   ],
   "source": [
    "#! SETUP 2\n",
    "_snlp_train_dir = _snlp_book_dir + \"/data/ohhla/train\"\n",
    "_snlp_dev_dir = _snlp_book_dir + \"/data/ohhla/dev\"\n",
    "_snlp_train_song_words = ohhla.words(ohhla.load_all_songs(_snlp_train_dir))\n",
    "_snlp_dev_song_words = ohhla.words(ohhla.load_all_songs(_snlp_dev_dir))\n",
    "assert(len(_snlp_train_song_words)==1041496)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to file encoding issues this code produces one error `Could not load ...`. **Ignore this error**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Task 1</font>: Develop and Train the Model\n",
    "\n",
    "This is the core part of the assignment. You are to code up, train and tune a language model. Your language model needs to be subclass of the `lm.LanguageModel` class. You can use some of the existing language models developed in the lecture, or develop your own extensions. \n",
    "\n",
    "Concretely, you need to return a better language model in the `create_lm` function. This function receives a target vocabulary `vocab`, and it needs to return a language model defined over this vocabulary. \n",
    "\n",
    "The target vocab will be the union of the training and test set (hidden to you at development time). This vocab will contain words not in the training set. One way to address this issue is to use the `lm.OOVAwareLM` class discussed in the lecture notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## You should improve this cell\n",
    "import collections\n",
    "\n",
    "class NGramLM(lm.CountLM):\n",
    "    \"\"\"\n",
    "    Change the NGram language model to keep track of the various counts\n",
    "    needed for Knesser Ney\n",
    "    \"\"\"\n",
    "    def __init__(self, train, order):\n",
    "        \n",
    "        # Initialisation\n",
    "        super().__init__(set(train), order)\n",
    "        self._counts = collections.defaultdict(float)\n",
    "        self._norm = collections.defaultdict(float)\n",
    "        self._nbis = 0\n",
    "        self._nconts = collections.defaultdict(float) #N+ continuations\n",
    "        self._nhists = collections.defaultdict(float) #N+ histories\n",
    "        self._nmids = collections.defaultdict(float) #N+ (*t*)\n",
    "        self._nends = collections.defaultdict(float) \n",
    "        \n",
    "        for i in range(self.order, len(train)):\n",
    "            history = tuple(train[i - self.order + 1: i])\n",
    "            word = train[i]\n",
    "            if self._counts[(word,) + history] == 0.0:\n",
    "                self._nbis += 1.0\n",
    "                self._nconts[history] += 1.0\n",
    "                self._nhists[(word, )] += 1.0\n",
    "                if self.order > 2:\n",
    "                    self._nmids[tuple(history[-(self.order - 2):])] += 1.0\n",
    "                    self._nends[(word,) + tuple(history[-(self.order - 2):])] += 1.0\n",
    "            self._counts[(word,) + history] += 1.0\n",
    "            self._norm[history] += 1.0\n",
    "\n",
    "    def counts(self, word_and_history):\n",
    "        return self._counts[word_and_history]\n",
    "\n",
    "    def norm(self, history):\n",
    "        return self._norm[history]\n",
    "    \n",
    "class KneserNeyNGram(lm.LanguageModel):\n",
    "    \"\"\"\n",
    "    Kneser-Ney N-Gram model - recursively goes from highest order with fixed discount d\n",
    "    \"\"\"\n",
    "    def __init__(self, train, order, d):\n",
    "        \n",
    "        # Initialisation\n",
    "        super().__init__(set(train), order)\n",
    "        self.train = train\n",
    "        self.nmodel = self.order #the order of the n-gram model as we recurse down\n",
    "        self.d = d\n",
    "        self.ngram = collections.defaultdict(float) #collection of n-gram models\n",
    "        \n",
    "        for i in range(2, order + 1):\n",
    "            self.ngram[i] = NGramLM(self.train, i)\n",
    "    \n",
    "    def probability (self, word, *history):\n",
    "        model_hist = tuple(history[-(self.nmodel- 1):])\n",
    "        \n",
    "        if self.nmodel == 1:\n",
    "            self.nmodel = self.order\n",
    "            return self.ngram[2]._nhists[(word, )] / self.ngram[2]._nbis\n",
    "        \n",
    "        elif self.nmodel == self.order:\n",
    "            norm = self.ngram[self.nmodel].norm(model_hist)\n",
    "            \n",
    "            if not(norm): norm = len(self.vocab)\n",
    "            prob = max(self.ngram[self.nmodel].counts((word, ) + model_hist) - self.d, 0) / norm\n",
    "        \n",
    "        elif self.nmodel > 1 and self.nmodel != self.order:\n",
    "            norm = self.ngram[self.nmodel + 1]._nmids[model_hist]\n",
    "            \n",
    "            if not(norm): norm = len(self.vocab)\n",
    "            prob = max(self.ngram[self.nmodel + 1]._nends[(word, ) + model_hist] - self.d, 0) / norm\n",
    "        \n",
    "        #calc lambda\n",
    "        nconts = self.ngram[self.nmodel]._nconts[model_hist]\n",
    "        if nconts > 0:\n",
    "            _lambda = self.d / norm * nconts\n",
    "        else:\n",
    "            if norm != len(self.vocab):\n",
    "                _lambda = self.d / norm * len(self.vocab)\n",
    "            else:\n",
    "                _lambda = 1.0\n",
    "        self.nmodel -= 1\n",
    "        return prob + (_lambda * self.probability(word, *history))\n",
    "        \n",
    "def create_lm(vocab):\n",
    "    \"\"\"\n",
    "    Return an instance of `lm.LanguageModel` defined over the given vocabulary.\n",
    "    Args:\n",
    "        vocab: the vocabulary the LM should be defined over. It is the union of the training and test words.\n",
    "    Returns:\n",
    "        a language model, instance of `lm.LanguageModel`.\n",
    "    \"\"\"    \n",
    "    oov_train = lm.inject_OOVs(_snlp_train_song_words+_snlp_dev_song_words)\n",
    "    oov_vocab = set(oov_train)\n",
    "    missing_words = set([word for word in _snlp_vocab if word not in oov_vocab])\n",
    "\n",
    "    #Cross validation and optimisation done below...the optimum parameter comes from this          \n",
    "    return  lm.OOVAwareLM(KneserNeyNGram(oov_train,6,0.914522743225), missing_words) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='green'>Setup 3</font>: Specify Test Data\n",
    "This cell defines the directory to load the test songs from. When we evaluate your notebook we will point this directory elsewhere and use a **hidden test set**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#! SETUP 3\n",
    "_snlp_test_dir = _snlp_book_dir + \"/data/ohhla/dev\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='green'>Setup 4</font>: Load Test Data and Prepare Language Model\n",
    "In this section we load the test data, prepare the reference vocabulary and then create your language model based on this vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#! SETUP 4\n",
    "_snlp_test_song_words = ohhla.words(ohhla.load_all_songs(_snlp_test_dir))\n",
    "_snlp_test_vocab = set(_snlp_test_song_words)\n",
    "_snlp_dev_vocab = set(_snlp_dev_song_words)\n",
    "_snlp_train_vocab = set(_snlp_train_song_words)\n",
    "_snlp_vocab = _snlp_test_vocab | _snlp_train_vocab | _snlp_dev_vocab\n",
    "_snlp_lm = create_lm(_snlp_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Assessment 1</font>: Test Normalization (20 pts)\n",
    "Here we test whether the conditional distributions of your language model are properly normalized. If probabilities sum up to $1$ you get full points, you get half of the points if probabilities sum up to be smaller than 1, and 0 points otherwise. Due to floating point issues we will test with respect to a tolerance $\\epsilon$ (`_eps`).\n",
    "\n",
    "Points:\n",
    "* 10 pts: $\\leq 1 + \\epsilon$\n",
    "* 20 pts: $\\approx 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum: 0.9999999999998268, ~1: True, <=1: True\n",
      "Sum: 0.9999999999999071, ~1: True, <=1: True\n",
      "Sum: 0.9999999999991043, ~1: True, <=1: True\n"
     ]
    }
   ],
   "source": [
    "#! ASSESSMENT 1\n",
    "_snlp_test_token_indices = [100, 1000, 10000]\n",
    "_eps = 0.000001\n",
    "for i in _snlp_test_token_indices:\n",
    "    result = sum([_snlp_lm.probability(word, *_snlp_test_song_words[i-_snlp_lm.order+1:i]) for word in _snlp_vocab])\n",
    "    print(\"Sum: {sum}, ~1: {approx_1}, <=1: {leq_1}\".format(sum=result, \n",
    "                                                            approx_1=abs(result - 1.0) < _eps, \n",
    "                                                            leq_1=result - _eps <= 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above solution is marked with **\n",
    "<!-- ASSESSMENT 2: START_POINTS -->\n",
    "20\n",
    "<!-- ASSESSMENT 2: END_POINTS --> \n",
    "points **."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>Assessment 2</font>: Apply to Test Data (50 pts)\n",
    "\n",
    "We assess how well your LM performs on some unseen test set. Perplexities are mapped to points as follows.\n",
    "\n",
    "* 0-10 pts: uniform perplexity > perplexity > 550, linear\n",
    "* 10-30 pts: 550 > perplexity > 140, linear\n",
    "* 30-50 pts: 140 > perplexity > *Best-Result*, linear\n",
    "\n",
    "The **linear** mapping maps any perplexity value between the lower and upper bound linearly to a score. For example, if uniform perplexity is $U$ and your model's perplexity is $P\\leq550$, then your score is $10\\frac{P-U}{550-U}$. \n",
    "\n",
    "The *Best-Result* perplexity is the minimum of the best perplexity the course organiser achieved, and the submitted perplexities.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.384279468112543"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.perplexity(_snlp_lm, _snlp_test_song_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above solution is marked with **\n",
    "<!-- ASSESSMENT 3: START_POINTS -->\n",
    "0\n",
    "<!-- ASSESSMENT 3: END_POINTS --> points**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Task 2</font>: Describe your Approach\n",
    "\n",
    "<p style=\"font-size:20px\">Goal\n",
    "\n",
    "The goal was to create a correctly normalised language model with as low a perplexity as possible on an unseen test set.\n",
    "\n",
    "<p style=\"font-size:20px\">OOVs\n",
    "\n",
    "Given that the test set was unseen and I was almost certain to encounter out of vocabulary words, I first used the OOVInject heuristic, then to correctly normalise the model for testing I used OOVAware for words in the overall vocabulary, but missing in training. \n",
    "\n",
    "<p style=\"font-size:20px\">NGrams\n",
    "\n",
    "I first experimented with NGrams. To avoid zero probabilities and thus infinite perplexity I used Laplace smooothing, finally I interpolated between higher and lower order models. At first I used the brute force optimiser provided by scipy. However as the order of the models became higher (I went to order 8), there were multiple parameters to tune (both laplace and interpolation parameters). So at this point I simply tuned by hand to get a rough idea of the perplexity. Even this simple method reduced perplexity on the development set to 171.\n",
    "\n",
    "<p style=\"font-size:20px\">Bar Aware\n",
    "\n",
    "The data has structure, particularly for [BAR]s. For example, [/BAR] was always followed by [BAR], plus they alternate. I also looked at the distribution of distances between [BAR]s. I made the model 'Bar Aware' by coding the most obvious rules. This reduced my perplexity from 171 to 159. \n",
    "\n",
    "<p style=\"font-size:20px\">Utilities\n",
    "\n",
    "For sensibility checks I used utility functions provided in the lectures such as plot_probabilities and particular sampling. Utilities plus cross-validation code are commented out at the end of this notebook. \n",
    "\n",
    "\n",
    "<p style=\"font-size:20px\">Kneser-Ney Models\n",
    "\n",
    "The 1999 paper by Chen and Goodman [1], gives compelling evidence for the efficacy of the Kneser-Ney model, and indeed the Modified KN model. I implemented first the bigram then the trigram version and finally the higher order recursive version of this model using a single discounting parameter (the more sophisticated modified, has different discounts between different orders). The intuition is that for lower order models, instead of using maximum likelihood we instead derive a contination probability. Another advantage is that the model only has one discount parameter to optimise. The graph of perplexity against this parameter is convex so I used bisection search to optimise.  \n",
    "\n",
    "\n",
    "<img src=\"https://i.imgsafe.org/c9dd46567d.png\">\n",
    "\n",
    "\n",
    "A trigram Kneser-Ney reduced the perplexity to 134. The chart above shows the perplexites for a model of order 6 which achieved a perplexity of 116. This also rendered the bar_aware model redundant, Mostly due to Kneser-Ney also picking up the continuation of [/BAR] with very high probability (see below). \n",
    "\n",
    "<img src=\"https://i.imgsafe.org/b432e386db.png\">\n",
    "\n",
    "\n",
    "The Kneser-Ney trigram model formula\n",
    "\n",
    "\n",
    "$P_{KN}\\left ( w_{3}| w_{1}w_{2}\\right ) = \\frac{max\\left \\{ c(w_{1}w_{2}w_{3})-D, 0 \\right \\}}{c(w_{1}w_{2})} + D * \\frac{\\mathrm{N}( w_{1}w_{2}\\bullet)}{c(w_{1}w_{2})} *   \\left(  \\frac{max \\left \\{ \\mathrm{N}( \\bullet w_{2}w_{3})-D,0 \\right \\}}{\\mathrm{N}( \\bullet w_{2}\\bullet)}   + D*\\frac{\\mathrm{N}( w_{2}\\bullet)}{\\mathrm{N}( \\bullet w_{2}\\bullet)}* \\frac{\\mathrm{N}( \\bullet w_{3})}{\\mathrm{N}(\\bullet \\bullet)}  \\right)$\n",
    "\n",
    "\n",
    "<p style=\"font-size:20px\">Generalisation\n",
    "\n",
    "For the unseen test set I wished to train the model on all the data available. I performed 5-fold cross validation in order to tune the model. I was happy to see that the optimum parameter varied little through each fold giving confidence that the model should generalise. However the perplexity did vary somewhat on each cross-validation hold out set. I chose the mean of the optimal parameters as my final Kneser-Ney model discounting parameter. The model was finally trained on all the data (train and development). In terms of the order of the model, although I was able to get lower perplexities (albeit at a reducing rate) with models up to order 8 (perplexity of 113), higher order models also seemed like overkill and more to the point did kill the kernel in Jupyter several times ! Thus my implementation settled on an order 6 Kneser-Ney model.\n",
    "\n",
    "<p style=\"font-size:20px\">Further work\n",
    "\n",
    "I suspect the results could have been further improved had I implemented the full modified Kneser-Ney. However, I slowed down rapidly with my fiddly KN trigram implementation and also would have had extra parameters to optimise. It would have been interesting to compare with a neural net language model.\n",
    "\n",
    "<p style=\"font-size:20px\">Reference\n",
    "\n",
    "[1] Chen Goodman http://www2.denizyuret.com/ref/goodman/chen-goodman-99.pdf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Assessment 3</font>: Assess Description (30 pts) \n",
    "\n",
    "We will mark the description along the following dimensions: \n",
    "\n",
    "* Clarity (10pts: very clear, 0pts: we can't figure out what you did)\n",
    "* Creativity (10pts: we could not have come up with this, 0pts: Use the unigram model from the lecture notes)\n",
    "* Substance (10pts: implemented complex state-of-the-art LM, 0pts: Use the unigram model from the lecture notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above solution is marked with **\n",
    "<!-- ASSESSMENT 1: START_POINTS -->\n",
    "0\n",
    "<!-- ASSESSMENT 1: END_POINTS --> points**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport statnlpbook.util as util\\ndef plot_probabilities(lm, context = (\\'sun\\',\\'[BAR]\\', \\'ain\\', \"\\'t\", \\'shining\\', \\'Tupac\\', \\'[/BAR]\\'), how_many = 10):    \\n    probs = sorted([(word,lm.probability(word,*context)) for word in lm.vocab], key=lambda x:x[1], reverse=True)[:how_many]\\n    util.plot_bar_graph([prob for _,prob in probs], [word for word, _ in probs])\\nplot_probabilities(_snlp_lm)\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Code from here is now commented out\n",
    "#Some utilities for sensibility checking \n",
    "\n",
    "#Given a history plot_probalities shows the next most likely word...in the case [/BAR] is the final word\n",
    "#I wish to see a high probability that [BAR] will be next\n",
    "\"\"\"\n",
    "import statnlpbook.util as util\n",
    "def plot_probabilities(lm, context = ('sun','[BAR]', 'ain', \"'t\", 'shining', 'Tupac', '[/BAR]'), how_many = 10):    \n",
    "    probs = sorted([(word,lm.probability(word,*context)) for word in lm.vocab], key=lambda x:x[1], reverse=True)[:how_many]\n",
    "    util.plot_bar_graph([prob for _,prob in probs], [word for word, _ in probs])\n",
    "plot_probabilities(_snlp_lm)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nhist = (\\'sun\\',\\'[BAR]\\', \\'ain\\', \"\\'t\", \\'shining\\', \\'Tupac\\', \\'[/BAR]\\')\\nimport numpy as np\\n\\ndef sample(lm, init, amount,test_vocab):\\n    \\n    #words = list(lm.vocab)\\n    words = list(test_vocab)\\n    result = []\\n    result += init\\n    for i in range(0, amount):\\n        history = result[-(lm.order-1):]\\n        probs = [lm.probability(word, *history) for word in words]\\n        sampled = np.random.choice(words,p=probs)\\n        result.append(sampled)\\n    return result\\n\\nsample(_snlp_lm,hist,10, _snlp_vocab)\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sampling utility to see how closely generated text matches text in the development set\n",
    "\"\"\"\n",
    "hist = ('sun','[BAR]', 'ain', \"'t\", 'shining', 'Tupac', '[/BAR]')\n",
    "import numpy as np\n",
    "\n",
    "def sample(lm, init, amount,test_vocab):\n",
    "    \n",
    "    #words = list(lm.vocab)\n",
    "    words = list(test_vocab)\n",
    "    result = []\n",
    "    result += init\n",
    "    for i in range(0, amount):\n",
    "        history = result[-(lm.order-1):]\n",
    "        probs = [lm.probability(word, *history) for word in words]\n",
    "        sampled = np.random.choice(words,p=probs)\n",
    "        result.append(sampled)\n",
    "    return result\n",
    "\n",
    "sample(_snlp_lm,hist,10, _snlp_vocab)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport matplotlib.pyplot as plt\\n\\noov_train = lm.inject_OOVs(_snlp_train_song_words)\\noov_vocab = set(oov_train)\\nmissing_words = set([word for word in _snlp_vocab if word not in oov_vocab])\\nalphas = np.arange(0.05,1.05,0.05)    \\n\\n\\nperplexities = [lm.perplexity(lm.OOVAwareLM(KneserNeyNGram(oov_train,6,alpha), missing_words),_snlp_dev_song_words) for alpha in alphas]\\nfig = plt.figure()\\nplt.plot(alphas,perplexities)\\nplt.ylabel(\"Perplexity\")\\nplt.xlabel(\"KNGram parameter\")\\nplt.savefig(\"perplexity.png\")\\nplt.show(fig)\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#View the perplexities on the development set given a varying parameter in KNTri\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "oov_train = lm.inject_OOVs(_snlp_train_song_words)\n",
    "oov_vocab = set(oov_train)\n",
    "missing_words = set([word for word in _snlp_vocab if word not in oov_vocab])\n",
    "alphas = np.arange(0.05,1.05,0.05)    \n",
    "\n",
    "\n",
    "perplexities = [lm.perplexity(lm.OOVAwareLM(KneserNeyNGram(oov_train,6,alpha), missing_words),_snlp_dev_song_words) for alpha in alphas]\n",
    "fig = plt.figure()\n",
    "plt.plot(alphas,perplexities)\n",
    "plt.ylabel(\"Perplexity\")\n",
    "plt.xlabel(\"KNGram parameter\")\n",
    "plt.savefig(\"perplexity.png\")\n",
    "plt.show(fig)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain_dev = _snlp_train_song_words+_snlp_dev_song_words\\ndef find_optimal(low, high, oov_train, test_set, epsilon=1e-6):\\n        \\n        print(high, low)\\n        if high - low < epsilon:\\n            return high, lm.perplexity(lm.OOVAwareLM(KneserNeyNGram(oov_train,6,high), missing_words),test_set)\\n        else:\\n            mid = (high+low) / 2.0\\n            left = lm.perplexity(lm.OOVAwareLM(KneserNeyNGram(oov_train,6,mid-epsilon), missing_words),test_set)\\n            right = lm.perplexity(lm.OOVAwareLM(KneserNeyNGram(oov_train,6,mid+epsilon), missing_words),test_set)\\n            if left < right:\\n                return find_optimal(low, mid, oov_train,test_set, epsilon)\\n            else:\\n                return find_optimal(mid, high,oov_train,test_set, epsilon)\\n\\nalphas_perps = []\\nfor j in range(4,-1,-1):\\n    \\n    train_set = train_dev[:j*len(train_dev)//5]+train_dev[(j+1)*len(train_dev)//5:]\\n    dev_set = train_dev[j*len(train_dev)//5:(j+1)*len(train_dev)//5]\\n    \\n    oov_train = lm.inject_OOVs(train_set)\\n    oov_vocab = set(oov_train)\\n    missing_words = set([word for word in _snlp_vocab if word not in oov_vocab])\\n    \\n    alpha = find_optimal(0.0,1.0,oov_train,dev_set)\\n    alphas_perps.append(alpha)\\n    print(alpha)\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For optimisation of the KN parameter, do cross validation\n",
    "\"\"\"\n",
    "train_dev = _snlp_train_song_words+_snlp_dev_song_words\n",
    "def find_optimal(low, high, oov_train, test_set, epsilon=1e-6):\n",
    "        \n",
    "        print(high, low)\n",
    "        if high - low < epsilon:\n",
    "            return high, lm.perplexity(lm.OOVAwareLM(KneserNeyNGram(oov_train,6,high), missing_words),test_set)\n",
    "        else:\n",
    "            mid = (high+low) / 2.0\n",
    "            left = lm.perplexity(lm.OOVAwareLM(KneserNeyNGram(oov_train,6,mid-epsilon), missing_words),test_set)\n",
    "            right = lm.perplexity(lm.OOVAwareLM(KneserNeyNGram(oov_train,6,mid+epsilon), missing_words),test_set)\n",
    "            if left < right:\n",
    "                return find_optimal(low, mid, oov_train,test_set, epsilon)\n",
    "            else:\n",
    "                return find_optimal(mid, high,oov_train,test_set, epsilon)\n",
    "\n",
    "alphas_perps = []\n",
    "for j in range(4,-1,-1):\n",
    "    \n",
    "    train_set = train_dev[:j*len(train_dev)//5]+train_dev[(j+1)*len(train_dev)//5:]\n",
    "    dev_set = train_dev[j*len(train_dev)//5:(j+1)*len(train_dev)//5]\n",
    "    \n",
    "    oov_train = lm.inject_OOVs(train_set)\n",
    "    oov_vocab = set(oov_train)\n",
    "    missing_words = set([word for word in _snlp_vocab if word not in oov_vocab])\n",
    "    \n",
    "    alpha = find_optimal(0.0,1.0,oov_train,dev_set)\n",
    "    alphas_perps.append(alpha)\n",
    "    print(alpha)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nparam = np.mean([x[0] for x in alphas_perps])\\nprint(param)\\nperp = np.mean([x[1] for x in alphas_perps])\\nprint(perp)\\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "param = np.mean([x[0] for x in alphas_perps])\n",
    "print(param)\n",
    "perp = np.mean([x[1] for x in alphas_perps])\n",
    "print(perp)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nclass BarAwareLM(lm.LanguageModel):\\n      \\n    def __init__(self,base_lm):\\n        \\n        super().__init__(base_lm.vocab,base_lm.order)\\n        self.base_lm = base_lm\\n        \\n    def probability(self, word, *history):\\n        if history[-1] == '[/BAR]':\\n            if word =='[BAR]':\\n                return 1.0\\n            else:\\n                return 0.0\\n        else:\\n            #Note that [Bar],[/Bar] alternate so adjust probabilities\\n            hist = history #[-(self.order - 1):]\\n            if '[BAR]' in hist and '[/BAR]' in hist:\\n                last_bar = max(loc for loc, val in enumerate(hist) if val == '[BAR]')\\n                last_slashbar = max(loc for loc, val in enumerate(hist) if val == '[/BAR]')\\n                if last_bar>last_slashbar:\\n                    if word =='[BAR]':\\n                        return 0.0\\n                    else:\\n                        return self.base_lm.probability( word, *history)/(1-self.base_lm.probability('[BAR]', *history))\\n                else:\\n                    if word =='[/BAR]':\\n                        return 0.0\\n                    else:\\n                        return self.base_lm.probability( word, *history)/(1-self.base_lm.probability('[/BAR]', *history)) \\n            elif '[BAR]' in hist:\\n                if word =='[BAR]':\\n                    return 0.0\\n                else:\\n                    return self.base_lm.probability( word, *history)/(1-self.base_lm.probability('[BAR]', *history))\\n            elif '[/BAR]' in hist:\\n                if word =='[/BAR]':\\n                    return 0.0\\n                else:\\n                    return self.base_lm.probability( word, *history)/(1-self.base_lm.probability('[/BAR]', *history)) \\n                    \\n            return self.base_lm.probability( word, *history)\\n\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Redundant Bar Aware Language model - no longer needed due to Knesser-Ney\n",
    "\"\"\"\n",
    "class BarAwareLM(lm.LanguageModel):\n",
    "      \n",
    "    def __init__(self,base_lm):\n",
    "        \n",
    "        super().__init__(base_lm.vocab,base_lm.order)\n",
    "        self.base_lm = base_lm\n",
    "        \n",
    "    def probability(self, word, *history):\n",
    "        if history[-1] == '[/BAR]':\n",
    "            if word =='[BAR]':\n",
    "                return 1.0\n",
    "            else:\n",
    "                return 0.0\n",
    "        else:\n",
    "            #Note that [Bar],[/Bar] alternate so adjust probabilities\n",
    "            hist = history #[-(self.order - 1):]\n",
    "            if '[BAR]' in hist and '[/BAR]' in hist:\n",
    "                last_bar = max(loc for loc, val in enumerate(hist) if val == '[BAR]')\n",
    "                last_slashbar = max(loc for loc, val in enumerate(hist) if val == '[/BAR]')\n",
    "                if last_bar>last_slashbar:\n",
    "                    if word =='[BAR]':\n",
    "                        return 0.0\n",
    "                    else:\n",
    "                        return self.base_lm.probability( word, *history)/(1-self.base_lm.probability('[BAR]', *history))\n",
    "                else:\n",
    "                    if word =='[/BAR]':\n",
    "                        return 0.0\n",
    "                    else:\n",
    "                        return self.base_lm.probability( word, *history)/(1-self.base_lm.probability('[/BAR]', *history)) \n",
    "            elif '[BAR]' in hist:\n",
    "                if word =='[BAR]':\n",
    "                    return 0.0\n",
    "                else:\n",
    "                    return self.base_lm.probability( word, *history)/(1-self.base_lm.probability('[BAR]', *history))\n",
    "            elif '[/BAR]' in hist:\n",
    "                if word =='[/BAR]':\n",
    "                    return 0.0\n",
    "                else:\n",
    "                    return self.base_lm.probability( word, *history)/(1-self.base_lm.probability('[/BAR]', *history)) \n",
    "                    \n",
    "            return self.base_lm.probability( word, *history)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
